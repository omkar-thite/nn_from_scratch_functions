{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89224581-c2af-4c00-a763-d5c98b1358ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eecb2e00-1d3b-466d-9e2e-78d730b6ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffffffd\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        '''\n",
    "        data - to be stored as value\n",
    "        _children - previous values that operated to produce current Value\n",
    "        _op - operation on previous two values that resulted in this Value\n",
    "        _backward - chains gradient of output with input, defined for _op nodes\n",
    "                - None for leaf (non _op) nodes \n",
    "        '''\n",
    "        \n",
    "        self.data = data\n",
    "        self._prev = set(_children)  # Values which resulted in current Value\n",
    "        self.grad = 0.0  # maintains the derivative of this Value wrt Loss \n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        self.label = label  #\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Value(data={self.data})'\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, _children=(self, other), _op='+')\n",
    "\n",
    "        # take out's grad and propagate it to self and other's grads\n",
    "        # called after forward prop, i.e. when + already executed and backprop will have out.grad already\n",
    "        def _backward():\n",
    "            # dL/dself = dL/dout * dout/dself, below in reversed order\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        # at Forward pass\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, _children=(self, other), _op='*')\n",
    "\n",
    "        # called at backward pass \n",
    "        def _backward():\n",
    "            # dL/self = dL/dout * dout/dself\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): #other * self (other is not Value)\n",
    "        return self * other  # here self(Value) object's __mul__ is called.    \n",
    "\n",
    "    def __pow__(self, other): \n",
    "        assert isinstance(other, (int, float)), 'only supporting int/float powers for now'\n",
    "        out = Value(self.data ** other, _children=(self,), _op=f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other-1)) * out.grad  # self.data is int or float, ** takes precedence over *\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __neg__(self): #-self\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        #t = np.tanh(x)\n",
    "        out = Value(t, _children=(self,), _op='tanh')\n",
    "\n",
    "        def _backward():\n",
    "            # self -> tanh -> out\n",
    "            # dL/dself = dL/dout * dout/dself\n",
    "            # we'll have dL/dout or out.grad when this func is called due to backprop\n",
    "            self.grad += out.grad * (1 - (t ** 2))\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), _children=(self,), _op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad  # d.grad is derivative propagated until out, out.data is obtained at forward prop\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def backward(self):\n",
    "            # build topo graph starting at self\n",
    "            topo = []\n",
    "            visited = set()\n",
    "            \n",
    "            def build_topo(v):\n",
    "                if v not in visited:\n",
    "                    visited.add(v)\n",
    "                    for child in v._prev:\n",
    "                        build_topo(child)\n",
    "                    topo.append(v)\n",
    "            \n",
    "            build_topo(self)                \n",
    "            self.grad = 1.0\n",
    "            for node in reversed(topo):\n",
    "                node._backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b5655b-d571-432b-abf6-ca096794fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0, label='a')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9985eb91-9fbb-4304-bc14-6b3e69f88a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Value(-3.0, label='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56829210-0403-43ef-9f70-56785bbb2c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08719583-cd79-42d7-9b14-6693ac15b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.__add__(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b859ed95-84ae-4ca1-af0f-03760fc4f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=40.107584470096704)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bcacf7e9-aad9-4a11-ae6b-9720496aa442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-100.21516894019341)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label='L'\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4eff721c-32e0-49ab-ae4a-37532c1e84f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=50.107584470096704)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a.__mul__(b)).__add__(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "087409ad-14e7-4b99-a74f-165c1fced5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Value(data=10.0), Value(data=40.107584470096704)}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._prev # Values that resulted in d (a*b and c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8bcd5b51-d45d-4c85-87fc-f7501c4902c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._op # operation on d._prev that resulted in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ad5c8a2-9598-4afe-a987-1dae1d933a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Value(data=-6.0), Value(data=0.8813735870195432), Value(data=50.107584470096704), Value(data=10.0), Value(data=0.0), Value(data=5.828427124746192), Value(data=-6.0), Value(data=40.107584470096704), Value(data=1.7627471740390863), Value(data=6.881373587019543), Value(data=0.0), Value(data=2), Value(data=2.0), Value(data=-3.0), Value(data=1.0)} {(Value(data=2.0), Value(data=-6.0)), (Value(data=-3.0), Value(data=-6.0)), (Value(data=1.0), Value(data=0.0)), (Value(data=40.107584470096704), Value(data=50.107584470096704)), (Value(data=6.881373587019543), Value(data=0.8813735870195432)), (Value(data=-6.0), Value(data=0.8813735870195432)), (Value(data=1.7627471740390863), Value(data=5.828427124746192)), (Value(data=5.828427124746192), Value(data=40.107584470096704)), (Value(data=-6.0), Value(data=-6.0)), (Value(data=10.0), Value(data=50.107584470096704)), (Value(data=0.0), Value(data=0.0)), (Value(data=0.8813735870195432), Value(data=1.7627471740390863)), (Value(data=2), Value(data=1.7627471740390863)), (Value(data=0.0), Value(data=-6.0)), (Value(data=6.881373587019543), Value(data=40.107584470096704))}\n"
     ]
    }
   ],
   "source": [
    "def trace(root):\n",
    "    # build a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "nodes, edges = trace(d)\n",
    "print(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "10fb1e8b-0655-4763-b15d-1fe9ef2afded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9999999999995595\n"
     ]
    }
   ],
   "source": [
    "def lol():\n",
    "    h = 0.001\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label = 'd'\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "\n",
    "    a = Value(2.0, label='a')\n",
    "    b = Value(-3.0 + h, label='b')\n",
    "    c = Value(10.0, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label = 'd'\n",
    "    f = Value(-2.0, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print((L2-L1)/h)\n",
    "    \n",
    "lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5960a489-111d-436f-bcdf-bb8e5f81fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.grad = 1.0\n",
    "f.grad = 4.0\n",
    "d.grad = -2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb64a9-5dfd-456f-8090-d71422afcd8f",
   "metadata": {},
   "source": [
    "'+' node doesn't know about rest of the graph, all it knows is that it took c and e and result in d. so it only knows the derivative of d wrt c and e i.e. local derivative. local derivative of plus node is 1.0 wrt its operands in graph (operands are standalone values, scalers). So plus node literally routes (lets pass) the gradients backwards when using chain rule. (whatever the derivative of result variable wrt L is same of derivative of operands (here childrens of + node) wrt L due to chain rule.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95d53baf-652f-4b87-ae8a-126c9ea59baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad = -2.0\n",
    "e.grad = -2.0\n",
    "\n",
    "a.grad = -2 * -3\n",
    "b.grad = -2 * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8993749f-a8b2-49d2-9090-5ff8e36b8020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, -4.0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83b1d17-7769-4879-bba9-236947644eeb",
   "metadata": {},
   "source": [
    "childrens -> ops -> output\n",
    "we always have derivative of output wrt L (as we go backwards),  \n",
    "we know local derivative of op wrt children so we get derivative of children wrt L,  \n",
    "Now those childrens are output of some other op backward.  \n",
    "We do this recursively to get derivatives of all nodes wrt L going backwards.  \n",
    "This is recursive application of chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47744c-f289-44a1-91a0-465e1c78945b",
   "metadata": {},
   "source": [
    "We change a, b, c, f in drection of their gradients by little, which results in L going up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d7f67004-65c6-4fd8-aed3-cf9e81516886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-98.51926240864292)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data += 0.01 * a.grad # Increased in direction of gradient by small amount controlled by step_size 0.01\n",
    "b.data += 0.01 * b.grad\n",
    "c.data += 0.01 * c.grad\n",
    "f.data += 0.01 * f.grad\n",
    "\n",
    "e = a*b\n",
    "d = e + c\n",
    "L = d*f\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72787c4-63c5-407e-9bd8-a8e996b32298",
   "metadata": {},
   "source": [
    "This way we can influence the final output\n",
    "bias b is like trigger of happyness to the neuron, it can make neuron more or less happy regardless of the input. (think biologizally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "281ccf84-6517-47dd-a2e0-c4a20299c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize tanh\n",
    "plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0ed7c-8955-4fd9-9f73-e80a1c3a87ff",
   "metadata": {},
   "source": [
    "- Very positive caps to 1, very negative caps to -1\n",
    "- Weights are like synapses to neuron.\n",
    "- You can define arbitrary function as op without being atomic given that you know the local derivative of that function.\n",
    "  - eg. tanh implmented using np.exp function without explicitly defining our own exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aed69d-ac7c-41b8-92a0-471187a281ba",
   "metadata": {},
   "source": [
    "derivative of tanh(x) = 1 - tanh^(x)**2\n",
    "- if o = tanh(x) then do/dx = 1 - o**2 : local derivative of tanh\n",
    "\n",
    "- '+' is distributor of gradient from right to left\n",
    "- local derivative of * is the other term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69af69-f2e1-40bc-898f-ce9fe303973b",
   "metadata": {},
   "source": [
    "### Using _backward to backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e79df7cf-b3dd-4022-8662-306eb6d046ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label='L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "86c6fcd1-a8b3-4c98-8192-2a7f4d0caeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass \n",
    "\n",
    "# base case, by default this is set to 0\n",
    "L.grad = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0477b4d6-02bc-4a90-83a2-d60b2fd2d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "L._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a2cef5de-5a8d-4875-b280-ba9e1528af2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.0, 4.0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad, f.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7aad1038-a9ae-4e2c-8c35-d0a9440e2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f is just a scaler value no need to backprop it\n",
    "f._backward() # Nothing happens here, None is returned\n",
    "d._backward()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fc8ca776-3fc4-4be2-8c2c-be483ad3f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.0, -2.0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.grad, c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6faa12bf-3fb9-4c12-9f2d-18b7f82f28aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1ed28fc4-36fc-4b9d-b600-e3099489d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "e._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "57279081-63ca-49db-b36f-13521231c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.0, -4.0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fefd59b-c139-4a8c-9937-36e5bb8ee254",
   "metadata": {},
   "source": [
    "Thus we have implemented backprop using _backward() of each op node.  \n",
    "We also want to automate that:  \n",
    "We want to call _backward on node when all its previous backprop is done.  \n",
    "This order of graph can be achieved using what is called *'topological sort'*.  \n",
    "It's laying out of graph such that all the edges goes from only left to right.  \n",
    "Below we implement topological sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "47f45145-a0c3-46f3-91ac-5f5f58b23f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=2.0),\n",
       " Value(data=-3.0),\n",
       " Value(data=-6.0),\n",
       " Value(data=10.0),\n",
       " Value(data=4.0),\n",
       " Value(data=-2.0),\n",
       " Value(data=-8.0)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build topologicl graph\n",
    "topo = []\n",
    "visited = set() # Maintain vissited nodes\n",
    "\n",
    "# Starts at root, goes throught all its childern, lay them out from left to right\n",
    "# if child not visited, marks it as visited\n",
    "# Adds itself after visiting all its childrens\n",
    "# L is going to be added in topo after all of the cildrens have been procesed\n",
    "# this function guarantees that your in list once all childrens are in list\n",
    "def build_topo(v):\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)\n",
    "\n",
    "build_topo(L)\n",
    "topo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c35a9-d1a3-4283-be18-a676bddab28e",
   "metadata": {},
   "source": [
    "This has ordered our Value objects from right to left.  \n",
    "-8.0 is L. All its childrens are in list then it's added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8eda15be-375a-4e76-bdac-c8d35a28a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients \n",
    "\n",
    "# Forward pass\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label='L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e1aa9fd0-f09b-4a67-8510-205819a96309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for node in [a, b, c, e, d, f, L]:\n",
    "    print(node.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b101c01-0825-4485-96f6-68ab30834ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "-4.0\n",
      "-2.0\n",
      "-2.0\n",
      "-2.0\n",
      "4.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "\n",
    "def build_topo(v):\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)\n",
    "\n",
    "build_topo(L)\n",
    "\n",
    "# backprop \n",
    "\n",
    "# base case \n",
    "L.grad = 1.0\n",
    "\n",
    "# go left to right\n",
    "for node in reversed(topo):\n",
    "    node._backward()\n",
    "\n",
    "for node in [a, b, c, e, d, f, L]:\n",
    "    print(node.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228378dc-9be2-4682-aaf4-8369e44b9d8a",
   "metadata": {},
   "source": [
    "Finally we hide this functionality in Value class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cf0fddeb-bb21-4220-bfc4-ec007db2b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Reset gradients \n",
    "\n",
    "# Forward pass\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d*f; L.label='L'\n",
    "\n",
    "for node in [a, b, c, e, d, f, L]:\n",
    "    print(node.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "818dd72e-2b79-4561-aebd-5b85dd72fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "071d1c8a-1f5a-415a-b719-27af052ecb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "-4.0\n",
      "-2.0\n",
      "-2.0\n",
      "-2.0\n",
      "4.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for node in [a, b, c, e, d, f, L]:\n",
    "    print(node.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca295731-208c-4566-8088-944964c2c058",
   "metadata": {},
   "source": [
    "This was backprop for one neuron. **But there is a bug**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c1a1d026-bc3c-47f0-8ffa-d6df75a44df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 1.0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(data=3.0, label='a')\n",
    "b = a + a; b.label='b'\n",
    "b.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91163754-6055-4293-8e8c-1f5df11df55a",
   "metadata": {},
   "source": [
    "This gradient is not correct.  \n",
    "By calculus a.grad should be 2.0  \n",
    "a used twice in expression, but in out __add__ we treat left a as 'self' object while right a which is same as left as 'other' object.  \n",
    "In its _backward(), self.grad is set to 1.0, but other.grad is also set to 1.0. Here we're overwriting grad of a.  \n",
    "i.e. a.grad = 1.0, a.grad = 1.0 this is happening.\n",
    "In this case, self and other are suposed to be exact same object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a1db7-1d23-4b1d-a9d4-fbaa8b06f5d8",
   "metadata": {},
   "source": [
    "This issue will arise anytime we use any variable more than once. Consider following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "05245b49-3b70-4d77-8a5e-9ed965425064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0, -8.0, 1.0, -6.0, 1.0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a, b -> e = a + b -> f = d * e  \n",
    "#      -> d = a * b ->\n",
    "a = Value(-2)\n",
    "b = Value(3)\n",
    "\n",
    "d = a * b\n",
    "e = a + b\n",
    "f = d * e\n",
    "\n",
    "f.backward()\n",
    "a.grad, b.grad, d.grad, e.grad, f.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2865f1-e71a-4758-b0ed-0f2123f58cad",
   "metadata": {},
   "source": [
    "This are not correct.  \n",
    "When going backwards, the + operater first correctly propagates gradient to a, b.  \n",
    "But then after d, * operator propagates other gradients to a, b, overwriting on previous ones.  \n",
    "**The solution is multivariable chain rule.**  \n",
    "The solution is we accumulate or add these gradients.  \n",
    "i.e. in Value class, we set in _backward: self.grad += local_grad * out.grad.  \n",
    "This way we add to previosuly stored grad of variable. Initially grad set to 0 so this is ok.  \n",
    "So we start grad at 0 and any contribution that flows backwards will simply add."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1553b-c7e7-400c-8424-1d88371de1a5",
   "metadata": {},
   "source": [
    "We'll now define tanh in terms of x.   \n",
    "We had define it in terms of math.exp() function and we wrote it's derivative based on analytical solution (like on paper).  \n",
    "We could implement exp() function ourself so that derivative of tanh will be chained with this function instead of explicitly stating derivative of tanh.  \n",
    "Doing this way will allow us to implement few more operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63d9a2-b309-49e7-82c6-3ab9d2b82e8a",
   "metadata": {},
   "source": [
    "Right now we cant add an int to Value object. Lets modify definition of Value so that if int passed as 'other' in __add__ it'll be converted to Value object first. Make sure int comes after + operand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2185509e-1f11-47ee-95ad-af1afe555cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2) \n",
    "a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b113516-c79d-4ee9-bc9e-ca27685343d1",
   "metadata": {},
   "source": [
    "Same goes for *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a7541c32-b6c2-4dae-98c1-f93a30f844f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcb01f-27c3-44b3-9bf8-d002b7603521",
   "metadata": {},
   "source": [
    "but this will raise error as python doesn't know how to do `2 * a`.  \n",
    "This is same as `2.__mul__(a)`. 2 is an `int` object while `a` is an `Value` object.  \n",
    "a is passed as other to the `int` class's `__mul__` method. And it's not defined there what to do if other is `Value` object.  \n",
    "Instead you can define `__rmul__()` in `Value`. This is kind of fallback.  \n",
    "If python can't do `2*a`, it'll check *if by any chance `a` knows how to multiply `2`. And that will be called into `__rmul__()`.\n",
    "If python can't do `2*a`, it'll check if there's an `__rmul__` in `Value` and is called if present.  \n",
    "Here order of `2*a` are swapped, `__rmul__` passes a as self and 2 as other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d1bee856-b3a4-448d-83d0-7d643f44e425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f22fae-1e6b-4b13-b50e-b59f80381692",
   "metadata": {},
   "source": [
    "stack overflow:\n",
    "When Python attempts to multiply two objects, it first tries to call the left object's __mul__() method. If the left object doesn't have a __mul__() method (or the method returns NotImplemented, indicating it doesn't work with the right operand in question), then Python wants to know if the right object can do the multiplication. If the right operand is the same type as the left, Python knows it can't, because if the left object can't do it, another object of the same type certainly can't either.\n",
    "\n",
    "If the two objects are different types, though, Python figures it's worth a shot. However, it needs some way to tell the right object that it is the right object in the operation, in case the operation is not commutative. (Multiplication is, of course, but not all operators are, and in any case * is not always used for multiplication!) So it calls __rmul__() instead of __mul__()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30fdbf-01b5-4463-9e80-5114eed1031e",
   "metadata": {},
   "source": [
    "Now we implment exp() operation of Value using math.exp().  \n",
    "We do this so that we have exp() operation in out backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5863d151-dc13-497b-bdac-afb7bbd5717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=7.38905609893065)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2)\n",
    "a.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3d001-2273-42f9-8de4-fe1e83fd4d2b",
   "metadata": {},
   "source": [
    "Now we implement division. But we define it bu shuffling some operations as follows:  \n",
    "a = Value(2), b = Value(3)  \n",
    "a / b = a * (1 / b) = a * (b**-1)  \n",
    "\n",
    "Thus we want to implement an operation `x**k` for some `int` or `float` `k` and we would like to differentitate it.  \n",
    "And as a special case `k=-1` will be a division.\n",
    "\n",
    "define division using special method `__truediv__(self, other):` which will be called when `self / other` is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "213f3a5f-4efb-4561-bb12-c05204498487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.5)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2)\n",
    "b = Value(4)\n",
    "a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad5d06-fd6c-49c6-90f5-ee17ad6d4645",
   "metadata": {},
   "source": [
    "Now overlaod - also using `__sub__`.  \n",
    "define subtraction as addition of negation:\n",
    "a - b = a + (-b)\n",
    "\n",
    "and define negation using `__neg__(self)` which returns `self * -1` where we already have implemented `*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "19de47d5-9045-49f3-ab60-6d2225d3447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-2)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2)\n",
    "b = Value(4)\n",
    "a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb21bdf-6fb1-48b8-9e53-d8c49f453e25",
   "metadata": {},
   "source": [
    "Now remimplemnt forward pass using explicit formula for tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2eadb220-ed5c-40ab-bc7d-d891c66664e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 0.4999999999999999,\n",
       " 0.4999999999999999,\n",
       " 0.4999999999999999,\n",
       " 0.4999999999999999,\n",
       " 0.4999999999999999,\n",
       " 0.0,\n",
       " 0.9999999999999998,\n",
       " 0.4999999999999999,\n",
       " -1.4999999999999996)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset gradients \n",
    "\n",
    "# Forward pass\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "c = x1 * w1; c.label='c' \n",
    "d = x2 * w2; d.label='d'\n",
    "e = c + d; d.label='d'\n",
    "n = e + b; n.label='n'\n",
    "\n",
    "#----\n",
    "o = n.tanh()\n",
    "#----\n",
    "o.label='o'\n",
    "o.backward()\n",
    "o.grad, n.grad, e.grad, d.grad, c.grad, b.grad, w2.grad, w1.grad, x2.grad, x1.grad\n",
    "\n",
    "#draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd98c0-3b0b-4cab-845f-174166c395b8",
   "metadata": {},
   "source": [
    "Now using explicit formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "04fe4d52-f917-4fe5-a02c-04f07f131ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 1.0, 0.5, -1.5)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset gradients \n",
    "\n",
    "# Forward pass\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "c = x1 * w1; c.label='c' \n",
    "d = x2 * w2; d.label='d'\n",
    "e = c + d; d.label='d'\n",
    "n = e + b; n.label='n'\n",
    "\n",
    "#----\n",
    "a = (2*n).exp()\n",
    "o = (a - 1) / (a + 1)\n",
    "#----\n",
    "o.label='o'\n",
    "o.backward()\n",
    "o.grad, n.grad, e.grad, d.grad, c.grad, b.grad, w2.grad, w1.grad, x2.grad, x1.grad\n",
    "\n",
    "#draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34931234-7c97-4d9e-b166-a212234edd3a",
   "metadata": {},
   "source": [
    "The level at which you imppplement operations is up to you, you can implment them upto level of + and * , or at level of tanh which is composite operations.  \n",
    "All that matters is that we have some input some output which is function of inputs in some way, as long as you can do forward pass and backward pass of that little operation, it doesnt matter what that operation is and how compposite it is.  \n",
    "If you can write the local grads, chanin the grads and continue backprop. So design of what those functios are its completely upto you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c83a2e-80d5-4eaa-bb14-ee8b98903fc6",
   "metadata": {},
   "source": [
    "Now do it in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d1ccc421-263a-4401-93fa-8bf3516b9ad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m2.0\u001b[39m])\u001b[38;5;241m.\u001b[39mdouble() ; x1\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0.0\u001b[39m])\u001b[38;5;241m.\u001b[39mdouble() ; x2\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True\n",
    "x1 = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n",
    "\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('___')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', x2.grad.item())\n",
    "print('x1', x2.grad.item())\n",
    "print('w1', x2.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ed315-74e9-40a7-bcdd-243ab922b731",
   "metadata": {},
   "source": [
    "Tensors are n dimensional arrays of scalars.  \n",
    "See its shape using `.shape` attribute  \n",
    "We're casting Tensors to be double as python by default used double for floating point numbers.  \n",
    "By dfault data type of Tensors will be float32. We cast here to duble i.e. float64 just as in python.\n",
    "\n",
    "Because above are leaf nodes, by defauly pytorch assumes that do not require gradients, So we explicitly tell pytorch that all these nodes requires gradient using `requires_graident = True`. By dafut these are `False` for leaf nodes like inputs of the network, for efficiency reasons in most common cases.  \n",
    "\n",
    "Just like `Value` obejct, `Tensor` object has `.data` and `.grad` attribute. Their `.item()` method takes single Tenosr of one element and returns just that element, stripping out the tensor.\n",
    "\n",
    "Above, `o` is the `tensor` and `.item()` will extract numerical value stored in it. (here 0.707...).\n",
    "tensor also has `backward()` method just as we implemented. Every tensor also has `.grad` i.e. gradients. We can pop oout the number using `.grad.items()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245ae13-738e-4740-9010-ec3d4d42daf4",
   "metadata": {},
   "source": [
    "`o.item()` and `o.data.item()` will produce the same result in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b65ed1-b49e-49bc-ad5f-3ef2764510c3",
   "metadata": {},
   "source": [
    "We can now start bulding neural nets. We define a neuron in similar way of how pytorch designs its neurons in its nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7b1690a2-93e9-4812-b662-8367b37243f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "56ef2247-3a12-4af6-8597-6c4c3d1cc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnnnnn\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        '''\n",
    "        nin - number of inputs to this neuron '''\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)] # Create weights for every one of the inputs nin.\n",
    "        self.b = Value(random.uniform(-1, 1))  # Controls the overall 'trigger happiness' of this neuron\n",
    "\n",
    "    def __call__(self, x): \n",
    "        # returns dot product of x with w's\n",
    "        \n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), start=self.b) \n",
    "        out = act.tanh() # Non linearity\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # convenient function to return list of current neuron's all parameters\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]  # list of all w's and b.   self.w is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a879a-6b91-4659-b19d-4481d2a4ce19",
   "metadata": {},
   "source": [
    "pytorch has similar parametrs() that returns parameters of tensors. here we do it for scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5e96d4ae-e749-41b6-8c3a-423f6af27289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.7487510234949275)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)  # for 2 dimensional\n",
    "n(x)\n",
    "\n",
    "# You get different result everytime you run this beacuse of randomness in w's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f8552-7395-45f9-b7f6-a15c1b46d724",
   "metadata": {},
   "source": [
    "`zip(self.w, x)` takes two iterators and create a new iterator that iterates over tuples of corresponding entries, such that  \n",
    "```python\n",
    "print(list(zip(self.w, x)))\n",
    ">>>\n",
    "[(Value(data=-0.7495835769305297), 2), (Value(data=0.5123887578748552), 3)]\n",
    "```\n",
    "First value in tuple is w and second is x repectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40549a4a-a0df-4e98-bcad-9965ad820c93",
   "metadata": {},
   "source": [
    "`def __call__(self, x)` This is called when Neuron object is called as in `n(x)` where `n` is a neuron.\n",
    "\n",
    "```python\n",
    "act = sum(wi*xi for wi, xi in zip(self.w, x)) + self.b \n",
    "```\n",
    "This works because `sum` sums up addition of all pairs wi, xi.  \n",
    "since wi is `Value` and xi is `float` or `int`, in `sum()`, `__add__` of `int` or `float` class is called.  \n",
    "But that class doesn't know how to add `Value` object to it.  \n",
    "So python looks if `Value` has defined `__radd__()` which means `Value` obejct is at right of `+` operand.  \n",
    "We do have define `__radd__` for `Value` class which simple returns `self + other` where other is now an `int`.  \n",
    "This calls `__add__` of `Value` passing `Value` as self and `int` as other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb3d18-2096-41aa-9c55-fe1f505e60a7",
   "metadata": {},
   "source": [
    "*`sum` takes a second optional argument called `start` which is value over which all elements are added*.  \n",
    "So for efficiency, we specif `start=self.b` so that all wi*xi will be added on top of that and we don;t have to separately add `self.b`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b42e24-b262-4040-bcc2-cca8143fb363",
   "metadata": {},
   "source": [
    "Next we define a layer of neurons.  \n",
    "A layer of neurons is a set of neurons evaluated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "567f938e-9dc7-4971-a6e9-7a2ecea479d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        '''\n",
    "        nin : dimension of input (total inputs)\n",
    "        nout : total number of neurons in current layer (dimension of output)\n",
    "        '''\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        x - input to layer\n",
    "        '''\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs  # for convinience get single neuron if #ouput is 1. \n",
    "                                                    # return lists outs if #output neurons > 1\n",
    "\n",
    "    # Collect all the parameters of a layer\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()] # single list comprehensions (nested for's)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16971cd6-f1ba-4d41-9398-afb017fc9858",
   "metadata": {},
   "source": [
    "Layer contains list of neurons. Number of neurons we have is input argument `nout`.  \n",
    "When called we reutrn `out` which is just neurons in list evaluated (called) independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "710dffa3-a5c6-4ae7-84d9-fc44ce211320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.26919915961602253),\n",
       " Value(data=-0.8565253635841719),\n",
       " Value(data=0.46176000197934997)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Layer(nin=2, nout=3)\n",
    "n(x)\n",
    "\n",
    "# nin = 2 because x has 2 elements. Here we get 3 neurons evaluaion as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cc754-ae6b-4030-a2f1-180f57a83b07",
   "metadata": {},
   "source": [
    "`nin` is the #elements coming from input so as to initialzie #weights in each neuron  \n",
    "`nout` is the #neurons in current layer, each will evaluate one value and whole output will be `nout` dimesnional.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48809232-c550-43ed-8e5a-271aa288bace",
   "metadata": {},
   "source": [
    "Now let's define MultiLayerPerceptron (MLP):  \n",
    "MLP is a layers feed each other sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cccfd92f-3299-485a-b846-071c2ec8c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, nin, nouts):\n",
    "        # size of neurons in each layer (input + other layers)\n",
    "        sz = [nin] + nouts\n",
    "        # Iterate over sizes and create Layer size for them\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24e3de-962c-41ed-8bb8-cee44f30ea33",
   "metadata": {},
   "source": [
    "- `nin`Number of inputs as before  \n",
    "- `nouts` list of all the `nout`, this list defines sizes of all the layers that we want in out MLP.\n",
    "- `__call__` calls all layers sequentially, feeding input to first layer, then it's ouput, to next layer and so on until we get final layer's output.  \n",
    "\n",
    "Note that for each layer, it's previous layer's output is its input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264c914-1038-4247-9cbe-c8b6616a80c4",
   "metadata": {},
   "source": [
    "Lets implement MLP as :  \n",
    "3 neurons input -> 4 neurons layer -> 4 neurons layer -> 1 neuron output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d45aa2c4-2e22-4bcb-bd4b-62875639a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8405096646132303)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input (should have 3 elements correcsponding to 3 neurons)\n",
    "x = [2.0, 3.0, -1.0]\n",
    "\n",
    "# MLP(#inputs, [size of each layer])\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# run MLP\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "33393f35-4e46-46d2-9c21-8c48472bf4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.3393122214461519),\n",
       " Value(data=-0.22736223241330555),\n",
       " Value(data=-0.4084047043402126),\n",
       " Value(data=0.18085217644195928),\n",
       " Value(data=0.06055873937857381),\n",
       " Value(data=-0.9209549406130084),\n",
       " Value(data=0.8723189539704386),\n",
       " Value(data=0.6789690271591673),\n",
       " Value(data=-0.5393785584853745),\n",
       " Value(data=0.6772880145288525),\n",
       " Value(data=-0.6056728716358728),\n",
       " Value(data=-0.21708314882118662),\n",
       " Value(data=0.7589726120191778),\n",
       " Value(data=-0.6634089749232364),\n",
       " Value(data=-0.7589303180769256),\n",
       " Value(data=0.010447964598380377),\n",
       " Value(data=-0.997636507177941),\n",
       " Value(data=-0.3337416040386165),\n",
       " Value(data=0.6608864153425518),\n",
       " Value(data=0.9824229077176931),\n",
       " Value(data=-0.6044930512239601),\n",
       " Value(data=-0.8122677648792194),\n",
       " Value(data=0.9145174517857348),\n",
       " Value(data=0.7975629294705224),\n",
       " Value(data=0.06208350970819354),\n",
       " Value(data=0.46099264797205985),\n",
       " Value(data=0.33802366820205143),\n",
       " Value(data=-0.4090335254169264),\n",
       " Value(data=-0.7542023854403044),\n",
       " Value(data=0.6343233751220436),\n",
       " Value(data=0.30755232755133655),\n",
       " Value(data=-0.8089588603540898),\n",
       " Value(data=-0.4641446666243063),\n",
       " Value(data=0.07504439546556818),\n",
       " Value(data=0.7529311995670125),\n",
       " Value(data=-0.2570471509323009),\n",
       " Value(data=0.09060893231834233),\n",
       " Value(data=-0.33483893719276314),\n",
       " Value(data=0.9946059802426197),\n",
       " Value(data=0.8599043000572903),\n",
       " Value(data=0.6885811237268613)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All weights and biases inside entire nn MLP\n",
    "n.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c8a91f50-3739-46a0-9e54-c60d411b8b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total parameters in MLP\n",
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b7006d32-0f9f-4ad4-90c4-7b93e533cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(n(x)) tot drwae whole MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b23aaa-b636-439d-a04d-de5d73135823",
   "metadata": {},
   "source": [
    "With micrograd we can backpropagate through all network to getting grads of w's and b's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ad2afaac-1e45-4dc7-9e36-c9c10c01d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999999999996664),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=0.9999999999996664)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dataset (similar to binary classifier)\n",
    "\n",
    "# 4 examples of 3 elements each\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] #desired targets\n",
    "\n",
    "# Let's see what nn curently thinks of this example :\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eda08e-c03f-4d9b-bde8-d3ac0876e11e",
   "metadata": {},
   "source": [
    "these are the outpuuts of the nn on those 4 examples xs.  \n",
    "the performance of nn is not good as predictions does not looks like targets.\n",
    "It should really resemble target labels ys. \n",
    "We update weights w's and b's so that nn make better predictions of desired targets.  \n",
    "\n",
    "Trick is to define a single number that meaures total performance of your nn, we call this single number the loss.\n",
    "\n",
    "The loss will be high when nn's predictions are far from target and we want to minimize the loss.\n",
    "We use mean squared error loss as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "891af73b-7dd2-4b33-aafa-431e07f18f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.1130365149481824e-25),\n",
       " Value(data=0.0),\n",
       " Value(data=0.0),\n",
       " Value(data=1.1130365149481824e-25)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ygt for y ground truths\n",
    "# Individual loss components for each exmaple\n",
    "[(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e8611-3b54-48d4-95fb-53b345cb0dc6",
   "metadata": {},
   "source": [
    "we take predictions and ground truths, pair them, subtract them and square them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea611a88-5c54-4a9a-b28e-322b872529cd",
   "metadata": {},
   "source": [
    "As we can see, those predictions that are near to ground truth have small loss and those which are far have high loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3927e6-a782-43e1-980c-d35fa6528fa0",
   "metadata": {},
   "source": [
    "Final loss will be sum of all these numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "779b9574-a4b0-4c04-a314-154d55cd857c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.2260730298963647e-25)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260b23c-ce39-468d-b283-edd07493702f",
   "metadata": {},
   "source": [
    "We want this loss to be near zero so predictions are very near to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bbfea921-eec1-4c0f-9c3a-423f006d3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181dbaef-f5b5-4ca5-a994-f36eedb8e417",
   "metadata": {},
   "source": [
    "With above line, backprop is happened across MLP and w's and b's now have grads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f5ed5f77-082c-4cb0-a117-dfdcc3bc60fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=14.46521934873515)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w of first layer first neuron\n",
    "n.layers[0].neurons[0].w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b18836dd-ae7b-4e09-86f1-67fd269cefbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.510646128207027"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient of that w because of backprop\n",
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac9ef0-bd82-4eea-908e-3058d29c9dc8",
   "metadata": {},
   "source": [
    "Since derivative is positive, the influence of this w in positive on loss, this w increases, loss increases. We have this info for all neurons and all their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0dd394d4-ad76-4fe0-a334-f91e77fc3166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.46521934873515"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see above neurons data\n",
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8a893708-e074-4283-b1c9-4055ed03dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf110712-4bc9-4b76-bc99-2e5db08a6de6",
   "metadata": {},
   "source": [
    "grads of input data are not usable as input data is fixed and it is a given in a problem. We don't change it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0773c513-ed04-44c1-9525-8a612aa196fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny update to parameters based on grad info\n",
    "\n",
    "for p in n.parameters():\n",
    "    p.data += -(0.01 * p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb84962-4682-481f-ba77-ce3ae5e59f0a",
   "metadata": {},
   "source": [
    "if grad is +ve, increasing w in dir of its grad will increase w as well as the loss.\n",
    "if grad is -ve, going in direction of grad will decrease the w, but if w is decreases, as grad is -ve, loss will increased.  \n",
    "Se want to go in opoosite direction of gradient to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "82cb251f-d50c-4ce6-a3af-a42b0b0e6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.54032581001722"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d3345c03-7274-4619-9680-1177c40518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data increased by bit. as grad was -ve, increasing w makes loss go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bc864aed-9f97-4a4e-b7ee-a3e866148e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.3417073902479911),\n",
       " Value(data=-0.460391035409648),\n",
       " Value(data=-0.4522643172012172),\n",
       " Value(data=-0.5680275655601227)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input (should have 3 elements correcsponding to 3 neurons)\n",
    "x = [2.0, 3.0, -1.0]\n",
    "\n",
    "# MLP(#inputs, [size of each layer])\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# run MLP\n",
    "n(x)\n",
    "\n",
    "# Example dataset (similar to binary classifier)\n",
    "\n",
    "# 4 examples of 3 elements each\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] #desired targets\n",
    "\n",
    "# Let's see what nn curently thinks of this example :\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "39efd047-87a0-4511-b637-309b30490591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.8500813802798\n",
      "1 3.5941195009469324\n",
      "2 3.817806766049877\n",
      "3 2.795212509035672\n",
      "4 1.1524800436715144\n",
      "5 0.3262226830963592\n",
      "6 0.06724724110795116\n",
      "7 0.027649831288717713\n",
      "8 0.01621148877732779\n",
      "9 0.010581451068870009\n",
      "10 0.0070159886942897445\n",
      "11 0.004596720007401912\n",
      "12 0.002947258452250125\n",
      "13 0.0018440355781614234\n",
      "14 0.0011277134468607377\n",
      "15 0.0006771579026596502\n",
      "16 0.0004016334970402597\n",
      "17 0.00023673415593389966\n",
      "18 0.00013943447461138063\n",
      "19 8.244883660908718e-05\n"
     ]
    }
   ],
   "source": [
    "# Iterate the process\n",
    "for k in range(20):\n",
    "    # Forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "\n",
    "    # Compute loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parametrs\n",
    "    for p in n.parameters():\n",
    "        p.data += -(0.05 * p.grad)\n",
    "\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa6b9c-12ef-473f-b327-6ecad7b6bd37",
   "metadata": {},
   "source": [
    "loss decreased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "99ef9af9-179d-4610-abaf-6b6dff7b1236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999987606950086),\n",
       " Value(data=-0.9909203093780423),\n",
       " Value(data=-0.9999105423453185),\n",
       " Value(data=0.9999928858758135)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9949b9-f855-4766-a1a2-80952f15bfa9",
   "metadata": {},
   "source": [
    "y pred is closer to targets now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df2550-171f-421f-b5b4-0904e277be86",
   "metadata": {},
   "source": [
    "loss is wobbly structure, big step can overstep local minima and end up at different structure that's completely different, that could destbilize training or even blow the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ee15ac2c-2711-4e7e-ab0c-b32cf952f34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-1.781999053633227),\n",
       " Value(data=0.35894405752487507),\n",
       " Value(data=4.351049063077288),\n",
       " Value(data=0.6329353704555232),\n",
       " Value(data=-1.318921626802422),\n",
       " Value(data=-3.102221064929652),\n",
       " Value(data=2.3311720233596844),\n",
       " Value(data=-0.3662489922049612),\n",
       " Value(data=-1.6184070048787178),\n",
       " Value(data=3.9948143471804465),\n",
       " Value(data=-1.132793938720549),\n",
       " Value(data=0.14766934387853325),\n",
       " Value(data=-5.225771888143727),\n",
       " Value(data=3.17964794338328),\n",
       " Value(data=-2.8308929892066614),\n",
       " Value(data=-1.8815790331824096),\n",
       " Value(data=0.15843614181333485),\n",
       " Value(data=-0.4216544930408924),\n",
       " Value(data=0.8796434031442569),\n",
       " Value(data=-0.3633558669557103),\n",
       " Value(data=-0.09449208562944893),\n",
       " Value(data=-0.3432370028732889),\n",
       " Value(data=-0.7367068698307493),\n",
       " Value(data=0.9532552835171642),\n",
       " Value(data=1.1298688899183704),\n",
       " Value(data=-0.9687974139424894),\n",
       " Value(data=1.5513788246826046),\n",
       " Value(data=0.9417953511488502),\n",
       " Value(data=0.18649717509860128),\n",
       " Value(data=-0.7121643256320136),\n",
       " Value(data=-1.0458937493599236),\n",
       " Value(data=-2.1919396874780146),\n",
       " Value(data=-2.9901798691133834),\n",
       " Value(data=0.9130492044555524),\n",
       " Value(data=1.0819074683932928),\n",
       " Value(data=-1.5631768946781834),\n",
       " Value(data=0.50082932348992),\n",
       " Value(data=1.3655259992784912),\n",
       " Value(data=-2.4965305358529695),\n",
       " Value(data=3.8811269415180294),\n",
       " Value(data=-0.3281731853724984)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36530d12-d67f-45c9-9a2c-e17d1f2cdf5d",
   "metadata": {},
   "source": [
    "this is the settngs of nn that cause predictions of it to be very close to targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7a0fb-fde2-4091-8268-a882c5576fc7",
   "metadata": {},
   "source": [
    "THERE IS A BUG IN OUT CODE: \n",
    "WE DIDN:T ZEORED OUT GRADS AFETR AN ITERATION...  \n",
    "At each iteration we calculate grad and weights accordingly,  \n",
    "at next iteration, we calculate fresh gradients but add it to previous gradient.  \n",
    "Remember in our code we've done `+=` to every grads, so when we do `backward()`, **new gradient is added to previous gradient** which is not what we want.   \n",
    "We want to update w based on gradient of its current position.  \n",
    "**So we zero out gradients after each iteration**.  \n",
    "This is `zero_grad()` in pytorch.  \n",
    "\n",
    "Code with bug worked above because this is very simple problem, it's very easy even for buggy nn to fit this data. Here, grads got accumulated over every pass which gave us massive step size, convergin fast to solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3470a143-794a-4aa2-8788-18797ee3f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.682736882818457\n",
      "1 3.940326984826172\n",
      "2 3.6473064326456965\n",
      "3 3.208315027095084\n",
      "4 2.700999165961764\n",
      "5 2.538611686160313\n",
      "6 2.824549822229459\n",
      "7 1.5978148961850824\n",
      "8 0.5704586084526988\n",
      "9 0.2532501073385463\n",
      "10 0.21453204015620458\n",
      "11 0.18529879944121425\n",
      "12 0.16246184683287168\n",
      "13 0.1441982405338574\n",
      "14 0.1293090276344174\n",
      "15 0.11697186999988697\n",
      "16 0.10660566560822657\n",
      "17 0.09778944669333116\n",
      "18 0.09021161211764978\n",
      "19 0.08363710623857798\n"
     ]
    }
   ],
   "source": [
    "# Input (should have 3 elements correcsponding to 3 neurons)\n",
    "x = [2.0, 3.0, -1.0]\n",
    "\n",
    "# MLP(#inputs, [size of each layer])\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# run MLP\n",
    "n(x)\n",
    "\n",
    "# Example dataset (similar to binary classifier)\n",
    "\n",
    "# 4 examples of 3 elements each\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] #desired targets\n",
    "\n",
    "# Let's see what nn curently thinks of this example :\n",
    "ypred = [n(x) for x in xs]\n",
    "\n",
    "# Iterate the process\n",
    "for k in range(20):\n",
    "    # Forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "\n",
    "    # Zero out all gradients\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "        \n",
    "    # Compute loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parametrs\n",
    "    for p in n.parameters():\n",
    "        p.data += -(0.05 * p.grad)\n",
    "\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942e85b-4126-45b8-8d5b-e6179e11e32d",
   "metadata": {},
   "source": [
    "Here we have converged slowly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2adcb563-0711-4ea5-b7e1-62197fe7ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8787941686520615),\n",
       " Value(data=-0.8463578520233822),\n",
       " Value(data=-0.8615539550239035),\n",
       " Value(data=0.8382191739375744)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba42251-f1a8-4340-a045-758e9038cbd3",
   "metadata": {},
   "source": [
    "nn can sometiems be tricky, you may have lots of bugs in your code but your network might still work. But if problem was very complex then this bug wouldn't have let us optimize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac950a-42a0-47e2-8c14-80f98ef693c9",
   "metadata": {},
   "source": [
    "Above iteration of the process is wht we call **gradient descent**.  \n",
    "The update to w's is what is called **stoachastic gradient descent update** where we update w's on evry example.  \n",
    "People use slighlt different updates.  \n",
    "loss can be different such as  cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47296cba-2e09-406e-bff7-967dd87df3a5",
   "metadata": {},
   "source": [
    "Lets match module API of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f5f1c721-bd0a-4a1a-a9cc-fcf5f1238130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00c007-ede7-4ba1-b077-5e8a5d890569",
   "metadata": {},
   "source": [
    "This will be the parent class of all classes we defined above.  \n",
    "`nn.Module` in pytorch also has `zero_grad()` which we refactored out above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8f38b-a986-422c-88c1-97544750027c",
   "metadata": {},
   "source": [
    "Visit repo to see full code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4533f2-3357-4a1e-826d-0f21e4cdc53c",
   "metadata": {},
   "source": [
    "To test, use pytorch and compare your forward and backward pass to pytorch's results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da52ad-ed79-4322-a409-643319382f04",
   "metadata": {},
   "source": [
    "If we have lots of data, we pick subset of it called the batch, only process that bacth, make forward, backward and update pass on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d646a45-9043-4268-a02e-50a3b12add3f",
   "metadata": {},
   "source": [
    "You can add any function to pytorch defined as class which should have forward pass and backward pass implemented. This way you can add your own lego block to castel of blocks that pytorch already has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75e4b5-c763-409b-bb8b-e05ee756595c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
